{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8154b2",
   "metadata": {},
   "source": [
    "The best learning hub is: https://www.tensorflow.org/\n",
    "\n",
    "__Goals__ of this tutorial: Understand the basic components of TensorFlow \n",
    "\n",
    "__Prerequisite__: Python, Numpy, Machine learning, Neural Network\n",
    "\n",
    "Materials to cover here: \n",
    "1. TensorFlow basics & fundamentals (Tensor, variable) (~20 mins)\n",
    "2. Preprocessing data (getting input data  into tensors) (~10 mins)\n",
    "3. Building and using pretrained deep learning models (saving and loading models) (~20 mins)\n",
    "4. Making predictions with a model on custom data (~10 mins)\n",
    "\n",
    "__Now, let's make some bugs together!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fe929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113a1ba",
   "metadata": {},
   "source": [
    "# 1.TensorFlow basics & fundamentals\n",
    "## 1.1 Tensor and Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc773d2",
   "metadata": {},
   "source": [
    "__Tensor:__ Similar to Numpy arrays, Tensors are __Multi-dimensioanl__ arrays of a uniform datatype. Unlike numpy arrays however, 1) Tensors are immutable; i.e. you can't update the contents of a tensor. 2)Tensors can be run on GPU/TPU which are much faster.  \n",
    "\n",
    "__Variable:__ contains a tensor that is persistent and changeable across different Session.runs. They are usually the ones that are updated in back-propagations (e.g. the weights of a model)\n",
    "\n",
    "For example, in Linear Regression:\n",
    "<center><img src=\"images/tensor_graph_session.png\" width=\"720\" height=\"360\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbee261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "dtype: <dtype: 'float32'>\n",
      "shape: (2, 2)\n",
      "ndim: 2\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "myTensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])  # Shift-Tab to open doc string of any function\n",
    "print(myTensor)\n",
    "print(\"dtype:\", myTensor.dtype)\n",
    "print(\"shape:\", myTensor.shape)\n",
    "print(\"ndim:\", myTensor.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44abc55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Zeros:\n",
      "  tf.Tensor([[0. 0. 0. 0. 0. 0.]], shape=(1, 6), dtype=float32)\n",
      "\n",
      " Ones:\n",
      "  tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      " Eights:  tf.Tensor(\n",
      "[[8 8 8 8]\n",
      " [8 8 8 8]\n",
      " [8 8 8 8]], shape=(3, 4), dtype=int32)\n",
      "\n",
      "\n",
      " normal:\n",
      "  tf.Tensor(\n",
      "[[ 0.3274685 -0.8426258]\n",
      " [ 0.3194337 -1.4075519]], shape=(2, 2), dtype=float32)\n",
      "\n",
      " uniform:\n",
      "  tf.Tensor(\n",
      "[[8]\n",
      " [3]], shape=(2, 1), dtype=int32)\n",
      "\n",
      " poisson:  tf.Tensor(\n",
      "[[3. 7.]\n",
      " [7. 6.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Some other common methods of instantiating tensors:\n",
    "\n",
    "# Constants\n",
    "zeros = tf.zeros(shape=(1,6))\n",
    "ones = tf.ones(shape=(2,3))\n",
    "eights = tf.constant(8, shape=(3,4))\n",
    "\n",
    "print(\"\\n Zeros:\\n \", zeros)\n",
    "print(\"\\n Ones:\\n \", ones)\n",
    "print(\"\\n Eights: \", eights)\n",
    "\n",
    "# Randomly sampled\n",
    "tf.random.set_seed(42)\n",
    "normal = tf.random.normal(shape=(2,2), mean=0, stddev=1.)\n",
    "uniform = tf.random.uniform(shape=(2,1), minval=0, maxval=10, dtype='int32')\n",
    "poisson = tf.random.poisson((2,2), 5)\n",
    "\n",
    "print(\"\\n\\n normal:\\n \", normal)\n",
    "print(\"\\n uniform:\\n \", uniform)\n",
    "print(\"\\n poisson: \", poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d86d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array:  [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Tensor:  tf.Tensor([ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.], shape=(12,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Converting between Numpy Arrays and Tensorflow Tensors\n",
    "\n",
    "# Numpy array to tensor\n",
    "\n",
    "Array = np.arange(12)\n",
    "print('Numpy array: ', Array)\n",
    "\n",
    "Tensor = tf.constant(Array, dtype=tf.float32)\n",
    "print('Tensor: ', Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e2e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to Numpy array \n",
    "\n",
    "Tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bb92bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([10,  7], dtype=int32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([10,  7], dtype=int32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the same tensor with tf.Variable() \n",
    "\n",
    "mutable_tensor = tf.Variable([10, 7])\n",
    "immutable_tensor = tf.constant([10, 7])\n",
    "\n",
    "mutable_tensor, immutable_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219a9092",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ResourceVariable' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-da15b2d26694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmutable_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m# need to use assign function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmutable_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ResourceVariable' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "mutable_tensor[0] = 5 # need to use assign function\n",
    "mutable_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12fcbd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([5, 7], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable_tensor[0].assign(5)\n",
    "mutable_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "immutable_tensor[0].assign(5)  # does not work as Tensor is immutable \n",
    "immutable_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35207a1",
   "metadata": {},
   "source": [
    "### Other useful things about Tensor that you can practice on your own. (very similar to numpy)\n",
    "__Tensor Indexing, Slicing, and Reshape, Tensor Math Operation, Aggregation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d436a9",
   "metadata": {},
   "source": [
    "#### a) A few basic math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd74334",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0, 2.0], \n",
    "                 [3.0, 4.0]])\n",
    "b = tf.constant([[1.0, 1.0], \n",
    "                 [1.0, 1.0]])\n",
    "\n",
    "\n",
    "print(tf.add(a, b), \"\\n\")\n",
    "print(tf.subtract(a,b), \"\\n\")\n",
    "print(tf.multiply(a, b), \"\\n\")\n",
    "print(tf.matmul(a, b), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalently\n",
    "print(a + b, \"\\n\") # element-wise addition\n",
    "print(a - b, \"\\n\") # element-wise subtraction\n",
    "print(a * b, \"\\n\") # element-wise multiplication\n",
    "print(a @ b, \"\\n\") # matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the absolute value\n",
    "abs_a = tf.abs(a)\n",
    "\n",
    "# Raising to a power\n",
    "pow_ba = tf.pow(a,b)\n",
    "\n",
    "print(\"\\n \", abs_a)\n",
    "print(\"\\n \", pow_ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8fe79",
   "metadata": {},
   "source": [
    "#### b) Indexing, Slicing and Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f80620",
   "metadata": {},
   "source": [
    "The indexing, slicing and reshaping rules are similar to NumPy.\n",
    "\n",
    "- index starts at 0\n",
    "- colons `:` are used for slices `start:stop:step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tf.constant([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(t1[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db654097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "\n",
    "print(tf.slice(t1,\n",
    "               begin=[1],\n",
    "               size=[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3cc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "\n",
    "x = tf.constant(np.arange(16))\n",
    "\n",
    "shape1 = [8,2]\n",
    "shape2 = [4,4]\n",
    "shape3 = [2,2,2,2]\n",
    "\n",
    "# Create Tensors of different shape\n",
    "\n",
    "a = tf.constant(x, shape=shape1)\n",
    "print(\"\\n a:\\n \", a)\n",
    "\n",
    "b = tf.constant(x, shape=shape2)\n",
    "print(\"\\n b:\\n \", b)\n",
    "\n",
    "c = tf.constant(x, shape=shape3)\n",
    "print(\"\\n c:\\n \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding rank of Tensors\n",
    "\n",
    "t1 = tf.expand_dims(c, 0)\n",
    "t2 = tf.expand_dims(c, 1)\n",
    "t3 = tf.expand_dims(c, 3)\n",
    "\n",
    "print(\"\\n After expanding dims:\\n t1 shape: \", t1.shape, \"\\n t2 shape: \", t2.shape, \"\\n t3 shape: \", t3.shape)\n",
    "\n",
    "\n",
    "# Squeezing redundant dimensions\n",
    "\n",
    "t1 = tf.squeeze(t1, 0)\n",
    "t2 = tf.squeeze(t2, 1)\n",
    "t3 = tf.squeeze(t3, 3)\n",
    "\n",
    "print(\"\\n After squeezing:\\n t1 shape: \", t1.shape, \"\\n t2 shape: \", t2.shape, \"\\n t3 shape: \", t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math ops will return a Tensor. Most Tensor ops will also apply to Variables.\n",
    "\n",
    "a = tf.Variable([[1.0, 2.0], \n",
    "                 [3.0, 4.0]])\n",
    "b = tf.Variable(tf.ones(shape=(2,2), dtype=tf.float32))\n",
    "\n",
    "# Element wise addition\n",
    "print( a + b )\n",
    "\n",
    "# Matrix multiplication\n",
    "print( a @ b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da94910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also name our variables\n",
    "\n",
    "x = tf.Variable([\"Hello world!\"], tf.string, name='string_var')\n",
    "y  = tf.Variable([3.14159, 2.71828], tf.float64, name='float_var')\n",
    "\n",
    "print(x.name)\n",
    "print(y.name)\n",
    "\n",
    "# By default, variables in models will acquire unique names automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9b3b2",
   "metadata": {},
   "source": [
    "__Note:__ In practice, raraly you need to decide whether to use tf.constant or tf.Variable, however, use tf.constant and change it later if needed if in doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154d32d",
   "metadata": {},
   "source": [
    "## 1.2 Automatic Differentiation and Regression in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213e3ea",
   "metadata": {},
   "source": [
    "#### Gradient Tapes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c494df",
   "metadata": {},
   "source": [
    "\" TensorFlow provides the __tf.GradientTape__ API for __automatic differentiation__; that is, computing the gradient of a computation with respect to some inputs, usually __tf.Variables__. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc45895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example\n",
    "\n",
    "x = tf.Variable(initial_value=5.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "    \n",
    "# dy/dx = 2x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "#print( dy_dx )\n",
    "print( dy_dx.numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96620d",
   "metadata": {},
   "source": [
    "__Gradient Tape tracks only trainable variables by default. However, it is also possible to track a constant tensor by calling `tape.watch()` on it:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(5.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.square(x)\n",
    "\n",
    "    \n",
    "# dy/dx = 2x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print( dy_dx.numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad2d8a",
   "metadata": {},
   "source": [
    "__GradientTape also works on list of inputs, and the inputs could be either scalars or high-dimensional tensors.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082487f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(1.0, name='w')\n",
    "b = tf.Variable(2.0, name='b')\n",
    "x = 5.\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape: #\n",
    "    y = (x * w + b)\n",
    "    z = y**2\n",
    "\n",
    "\n",
    "[dz_dw, dz_db] = tape.gradient(z, [w, b])\n",
    "\n",
    "# dz_dw = 2x(x*w + b); dz_db = 2(x*w + b)\n",
    "print( dz_dw.numpy() )\n",
    "print( dz_db.numpy() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient has been calculated and done, even the variable is changed later, the gradient will not be updated \n",
    "w.assign(-10000000)\n",
    "[dz_dw, dz_db] = tape.gradient(z, [w, b])\n",
    "print(dz_dw.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df840fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's also possible to compute gradients w.r.t intermediate results\n",
    "\n",
    "dz_dy = tape.gradient(z, y)  # dz_dy = 2*(y) = 2*(x*w+b) = 2*(5*1+2) = 14\n",
    "dz_dy.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794054f",
   "metadata": {},
   "source": [
    "__It is even possible to compute second order derivatives with nested gradient tapes__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f79453",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "\\begin{align}\n",
    "\\ s &= \\frac{1}{2}at^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "then,\n",
    "\n",
    "$$ v = \\frac{ds}{dt} $$\n",
    "$$ a = \\frac{dv}{dt} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.Variable(0.)\n",
    "a = tf.constant(9.8)\n",
    "\n",
    "clock = tf.constant(2.); t.assign(clock)\n",
    "\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "        distance = (a/2)*(t**2)\n",
    "    \n",
    "    speed = inner_tape.gradient(distance, t)\n",
    "acceleration = outer_tape.gradient(speed, t)\n",
    "\n",
    "\n",
    "print(f'Speed at t={clock.numpy()} is {speed:.1f}')\n",
    "print(f'Acceleration at t={clock.numpy()} is {acceleration:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706c44f",
   "metadata": {},
   "source": [
    "### Back to Linear Regression: An end to end example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = np.arange(0,70, 1, dtype='float64')\n",
    "Y = 2*X\n",
    "noise = 40*np.random.rand(70) \n",
    "Y = 2*X + noise\n",
    "\n",
    "plt.scatter(X, Y) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03359457",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.random.randn(1)\n",
    "b_init = np.random.randn(1)\n",
    "\n",
    "w = tf.Variable(initial_value=w_init)\n",
    "b = tf.Variable(initial_value=b_init)\n",
    "X = tf.constant(X)\n",
    "Y = tf.constant(Y)\n",
    "\n",
    "alpha, epochs = 0.0001, 10000   # The learning Rate, The number of iterations to perform gradient descent\n",
    "\n",
    "# Training the Neural Network\n",
    "for i in range(epochs): \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        Y_pred = w*X + b\n",
    "        Loss = tf.reduce_mean((Y - Y_pred )**2)\n",
    "        \n",
    "    # Compute Gradient\n",
    "    dL_dw, dL_db = tape.gradient(Loss, [w, b])\n",
    "    \n",
    "    # Update Parameters\n",
    "    w.assign_sub( alpha*dL_dw )\n",
    "    b.assign_sub( alpha*dL_db )\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print( \"Epoch {}, Loss : {}\".format(i, Loss.numpy()))\n",
    "print( \"Epoch {}, Loss : {}\".format(i, Loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "Y_pred = w*X + b\n",
    "\n",
    "plt.scatter(X, Y) \n",
    "plt.plot([X[0], X[-1]], [Y_pred[0], Y_pred[-1]], color='red')  # regression line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1561dd5",
   "metadata": {},
   "source": [
    "## 1.3 Eager Execution (Default in TF 2.0) vs. Graph Execution & Session in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f84161",
   "metadata": {},
   "source": [
    "The words “Tensor” and “Flow” mean that tensors (data) flow through the computational __graph__. (Default in TF1.0)\n",
    "\n",
    "What are __graphs__?\n",
    "\n",
    "In the previous codes, you ran TensorFlow eagerly. This means interactively TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\n",
    "\n",
    "There are no graphs in Eager execution, we can’t have the graph magic (automatic differentiation). We have to rely on tf.GradientTape to record operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93df1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly() # Default mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b40bf",
   "metadata": {},
   "source": [
    "### __The speed comparison of Eager vs Graph model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf589416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager time: 1.6753977700136602\n",
      "Graph time: 0.36331103404518217\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow imports\n",
    "import timeit\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Model building\n",
    "inputs = Input(shape=(28, 28)) \n",
    "x = Flatten()(inputs) \n",
    "x = Dense(256, \"relu\")(x)\n",
    "x = Dense(256, \"relu\")(x) \n",
    "x = Dense(256, \"relu\")(x) \n",
    "outputs = Dense(10, \"softmax\")(x) \n",
    "\n",
    "input_data = tf.random.uniform([100, 28, 28])\n",
    "\n",
    "# Eager Execution\n",
    "eager_model = Model(inputs=inputs, outputs=outputs)\n",
    "print(\"Eager time:\", timeit.timeit(lambda: eager_model(input_data), number=1000))\n",
    "\n",
    "#Graph Execution \n",
    "graph_model = tf.function(eager_model) # Wrap the model with tf.function \n",
    "print(\"Graph time:\", timeit.timeit(lambda: graph_model(input_data), number=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc21ee1",
   "metadata": {},
   "source": [
    "Practically, when we use high-level API like Keras, The Keras mode is about defining the __graph__, __compile__, and then __run__ it in GPU later. However, if we need to build new functions and want to execute each step interactively, then we need to annotate functions with the @tf.function decorator. By annotating your functions you tell TensorFlow to run those operation in a optimized graph in the GPU. This generated graph will be run very efficiently into the GPU.\n",
    "\n",
    "A good reference if you are interested to learn more: https://towardsdatascience.com/tensorflow-2-1-a-how-to-a3e6400d0899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e31ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wmlce-v1.7.0]",
   "language": "python",
   "name": "conda-env-wmlce-v1.7.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a6ae93",
   "metadata": {},
   "source": [
    "## Eager Execution (Default in TF 2.0) vs. Graph Execution & Session in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb383f",
   "metadata": {},
   "source": [
    "The words “Tensor” and “Flow” mean that tensors (data) flow through the computational __graph__. (Default in TF1.0)\n",
    "\n",
    "What are __graphs__?\n",
    "\n",
    "In the previous codes, you ran TensorFlow eagerly. This means interactively TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\n",
    "\n",
    "There are no graphs in Eager execution, we can’t have the graph magic (automatic differentiation). We have to rely on tf.GradientTape to record operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc54e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.executing_eagerly() # Default mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd8e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ab149",
   "metadata": {},
   "source": [
    "### __The speed comparison of Eager vs Graph model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437bf5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 14:24:50.388926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 171 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager time: 1.924209255957976\n",
      "Graph time: 0.7454432990052737\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow imports\n",
    "import timeit\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Model building\n",
    "inputs = Input(shape=(28, 28)) \n",
    "x = Flatten()(inputs) \n",
    "x = Dense(256, \"relu\")(x)\n",
    "x = Dense(256, \"relu\")(x) \n",
    "x = Dense(256, \"relu\")(x) \n",
    "outputs = Dense(10, \"softmax\")(x) \n",
    "\n",
    "input_data = tf.random.uniform([100, 28, 28])\n",
    "\n",
    "# Eager Execution\n",
    "eager_model = Model(inputs=inputs, outputs=outputs)\n",
    "print(\"Eager time:\", timeit.timeit(lambda: eager_model(input_data), number=1000))\n",
    "\n",
    "#Graph Execution \n",
    "graph_model = tf.function(eager_model) # Wrap the model with tf.function \n",
    "print(\"Graph time:\", timeit.timeit(lambda: graph_model(input_data), number=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e0775",
   "metadata": {},
   "source": [
    "Practically, when we use high-level API like Keras, The Keras mode is about defining the __graph__, __compile__, and then __run__ it in GPU later. However, if we need to build new functions and want to execute each step interactively, then we need to annotate functions with the @tf.function decorator. By annotating your functions you tell TensorFlow to run those operation in a optimized graph in the GPU. This generated graph will be run very efficiently into the GPU.\n",
    "\n",
    "A good reference if you are interested to learn more: https://towardsdatascience.com/tensorflow-2-1-a-how-to-a3e6400d0899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fb57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635e1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.6.1]",
   "language": "python",
   "name": "conda-env-opence-v1.6.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
